{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization in Neural Networks\n",
    "\n",
    "## Regularization of NNs\n",
    "\n",
    "Does regularization make sense in the context of neural networks? <br/>\n",
    "\n",
    "Yes! We still have all of the salient ingredients: a loss function, overfitting vs. underfitting, and coefficients (weights) that could get too large.\n",
    "\n",
    "But there are now a few different flavors besides L1 and L2 regularization. (Note that L1 regularization is not common in the context of  neural networks.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating our target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-calibrating\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting and scaling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding our target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing our model\n",
    "\n",
    "# We want to predict probabilities, so we'll use a softmax activation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling\n",
    "\n",
    "# We want to use categorical crossentropy as our optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting and saving the history log\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on some test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing with reality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model_r = Sequential()\n",
    "\n",
    "n_input = X_train_s.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_r.add(Dense(n_hidden, input_dim=n_input, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "model_r.add(Dense(7, activation='softmax',\n",
    "                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "model_r.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_r = model_r.fit(X_train_s, y_train, validation_data=(X_test_s, y_test),\n",
    "                       epochs=10, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_r.history['loss'], label='Training loss')\n",
    "plt.plot(history_r.history['val_loss'], label='Testing loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a new regularization idea: Turn some neurons off during training. We'll assign probabilities of 'dropout' and then let fate decide.\n",
    "\n",
    "$\\rightarrow$ Why is this a good idea? *Is* it a good idea?\n",
    "\n",
    "Was this sort of regularization available to us before? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "model_d = Sequential()\n",
    "\n",
    "n_input = X_train_s.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_d.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_d.add(Dropout(rate=0.2))\n",
    "model_d.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "model_d.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "history_d = model_d.fit(X_train_s, y_train, validation_data=(X_test_s, y_test),\n",
    "                       epochs=20, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_d.history['loss'], label='Training loss')\n",
    "plt.plot(history_d.history['val_loss'], label='Testing loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_d.history['acc'][-1], history_d.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another idea is to try to terminate the training process early, even before some pre-specified number of epochs.\n",
    "\n",
    "$\\rightarrow$ Why is this a good idea? *Is* it a good idea?\n",
    "\n",
    "Was this sort of regularization available to us before? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model_es = Sequential()\n",
    "\n",
    "n_input = X_train_s.shape[1]\n",
    "n_hidden = n_input\n",
    "\n",
    "model_es.add(Dense(n_hidden, input_dim=n_input, activation='relu'))\n",
    "model_es.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "model_es.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=1e-08, patience=0, verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [early_stop]\n",
    "\n",
    "history_es = model_es.fit(X_train_s, y_train, validation_data=(X_test_s, y_test),\n",
    "                         epochs=20, batch_size=None, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_es.history['loss'], label='Training loss')\n",
    "plt.plot(history_es.history['val_loss'], label='Testing loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
